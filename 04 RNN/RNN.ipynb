{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y45yhDufI5Uo"
   },
   "source": [
    "# **RNN**\n",
    "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55i8db8uI5U3"
   },
   "source": [
    "### IMDB sentiment classification task\n",
    "\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. IMDB provided a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided.\n",
    "\n",
    "You can download the dataset from http://ai.stanford.edu/~amaas/data/sentiment/  or you can directly use \n",
    "\" from keras.datasets import imdb \" to import the dataset.\n",
    "\n",
    "#### Few points to be noted:\n",
    "#### Modules like SimpleRNN, LSTM, Activation layers, Dense layers, Dropout can be directly used from keras\n",
    "#### For preprocessing, you can use required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SozhvLNkI5U6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "#load the imdb dataset \n",
    "from keras.datasets import imdb\n",
    "import keras\n",
    "vocabulary_size = 5000\n",
    "\n",
    "'''\n",
    "Significance of the argument \"num_words\":\n",
    "num_words: integer or None. Words are ranked by how often they occur (in the training set) and\n",
    "only the num_words most frequent words are kept. \n",
    "Any less frequent word will appear as oov_char value in the sequence data. \n",
    "If None, all words are kept. Defaults to None, so all words are kept.\n",
    "\n",
    "'''\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seeing sizes of the dataset and what features they have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num=100\\nX_train=X_train[:num]\\ny_train=y_train[:num]\\nX_test=X_test[:num]\\ny_test=y_test[:num]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''num=100\n",
    "X_train=X_train[:num]\n",
    "y_train=y_train[:num]\n",
    "X_test=X_test[:num]\n",
    "y_test=y_test[:num]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NrilwfurI5VA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---review---\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "---label---\n",
      "1\n",
      "---review with words---\n",
      "[' ', 'this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', \"everyone's\", 'really', 'suited', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', ' ', 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', ' ', 'father', 'came', 'from', 'the', 'same', 'scottish', 'island', 'as', 'myself', 'so', 'i', 'loved', 'the', 'fact', 'there', 'was', 'a', 'real', 'connection', 'with', 'this', 'film', 'the', 'witty', 'remarks', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', 'bought', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', ' ', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', 'fly', ' ', 'was', 'amazing', 'really', 'cried', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', 'cry', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', ' ', 'to', 'the', 'two', 'little', ' ', 'that', 'played', 'the', ' ', 'of', 'norman', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', ' ', 'list', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', 'grown', 'up', 'are', 'such', 'a', 'big', ' ', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', ' ', 'for', 'what', 'they', 'have', 'done', \"don't\", 'you', 'think', 'the', 'whole', 'story', 'was', 'so', 'lovely', 'because', 'it', 'was', 'true', 'and', 'was', \"someone's\", 'life', 'after', 'all', 'that', 'was', ' ', 'with', 'us', 'all']\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#the review is stored as a sequence of integers. \n",
    "# These are word IDs that have been pre-assigned to individual words, and the label is an integer\n",
    "idx_to_inspect=0\n",
    "print('---review---')\n",
    "print(X_train[idx_to_inspect])\n",
    "print('---label---')\n",
    "print(y_train[idx_to_inspect])\n",
    "\n",
    "# to get the actual review\n",
    "word2id = imdb.get_word_index()\n",
    "# bug fixed using this https://github.com/udacity/AIND-NLP/pull/1/commits/ef40e1716232e1c2f07a78cfdcf1983d82c358f1\n",
    "id2word = {i+3: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in X_train[idx_to_inspect]])\n",
    "print('---label---')\n",
    "print(y_train[idx_to_inspect])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some minor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 2, 22, 2, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([4,3,1],[1,2,2,2,22,2,2],key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n",
      "Minimum review length: 14\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(\n",
    "len(max((X_train + X_test), key=len))))\n",
    "print('Minimum review length: {}'.format(\n",
    "len(min((X_test + X_test), key=len))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and other things\n",
    "https://stats.stackexchange.com/a/211415\n",
    "\n",
    "#### It's common in neural network training to pad all sequences to be as long as the longest one. This is done to simplify the code. However, the theory fully supports having sequences of different lengths.\n",
    "\n",
    "Just to make one thing clearer: the reason to pad the sequence is to make the code more efficient, both in the sense of memory and computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h6upWxEWI5VC"
   },
   "outputs": [],
   "source": [
    "#pad sequences (write your code here)\n",
    "from keras.preprocessing import sequence\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences have now been padded\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequences have now been padded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-RcCOpeNI5VF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 16)                784       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,801\n",
      "Trainable params: 160,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#design a RNN model (write your code)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN\n",
    "############################\n",
    "embedding_size=32\n",
    "##########################\n",
    "# From the definition of Keras documentation the Sequential model is a linear stack of layers.You\n",
    "# A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "model_rnn=Sequential()\n",
    "#########################\n",
    "# Add an Embedding layer expecting input vocab of size 500, and\n",
    "# output embedding dimension of size 32.\n",
    "# revisit great explanation of embedding layer here: https://stackoverflow.com/q/45649520\n",
    "model_rnn.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model_rnn.add(SimpleRNN(16,input_shape = (max_words, embedding_size), return_sequences=False,activation=\"relu\"))\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))\n",
    "print(model_rnn.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mention reason to choose that particular loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross entropy\n",
    "The metric I choose to optimize is BINARY CROSS ENTROPY.\n",
    "\n",
    "FIrstly, the classification problem has 2 classes, so I am using binary cross entropy.\n",
    "This is better than mean squared loss AND “classification error = correct classifications/TOTAL SAMPLES” because it also helps us consider how good/bad our prediction was for a particular sample.\n",
    "\n",
    "Eg: let’s say sample A has actual label L1 and our model outputs for A belonging to L1 = 0.1\n",
    "Eg: let’s say sample B has actual label L1 and our model outputs for B belonging to L1 = 0.49\n",
    "Now, we will probably classify both A and B as having label L1 (as 0.1 < 0.5 and 0.49 < 0.5). But, the misclassification is more severe for sample A than sample B.\n",
    "\n",
    "\n",
    "To account for cases like this, the ln() function in cross-entropy takes into account the closeness of a prediction and is a more granular way to compute error.\n",
    "\n",
    "\n",
    "Also, I read that cross entropy converges faster than MSE and has less local minima  candidates than MSE.\n",
    "\n",
    "\n",
    "### Adam\n",
    "I chose Adam because i read it has some properties like:-\n",
    "* Robust to the choice of hyper parameters.\n",
    "* Self-learns learning rate on a per-parameter basis. (as opposed to normal gradient descent)\n",
    "* I also read in a blog that it combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "InQ2TED3I5VI"
   },
   "outputs": [],
   "source": [
    "#train and evaluate your model\n",
    "#choose your loss function and optimizer and mention the reason to choose that particular loss function and optimizer\n",
    "# use accuracy as the evaluation metric\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "q6A9Q0xmI5VJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 58s 91ms/step - loss: 1.1054 - accuracy: 0.6014 - val_loss: 0.6327 - val_accuracy: 0.6550\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 55s 87ms/step - loss: 0.5950 - accuracy: 0.6980 - val_loss: 0.6017 - val_accuracy: 0.6668\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 49s 78ms/step - loss: 8030.4634 - accuracy: 0.7035 - val_loss: 0.6277 - val_accuracy: 0.6204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9abef37650>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_size = int(len(y_train)*0.2)\n",
    "num_epochs = 3\n",
    "X_valid, y_valid = X_train[:valid_size], y_train[:valid_size]\n",
    "X_train2, y_train2 = X_train[valid_size:], y_train[valid_size:]\n",
    "model_rnn.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YTXG__EmI5VM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6279 - accuracy: 0.6180\n",
      "Test accuracy: 0.6180400252342224\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model using model.evaluate()\n",
    "scores = model_rnn.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results also for comparison with LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rnn=model_rnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1uSo8DgI5VN"
   },
   "source": [
    "# **LSTM**\n",
    "\n",
    "Instead of using a RNN, now try using a LSTM model and compare both of them. Which of those performed better and why ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Bk4rLYHwI5VP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size=32\n",
    "model_lstm=Sequential()\n",
    "model_lstm.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model_lstm.add(LSTM(100))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and evaluate your model\n",
    "#choose your loss function and optimizer and mention the reason to choose that particular loss function and optimizer\n",
    "# use accuracy as the evaluation metric\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 145s 231ms/step - loss: 0.4568 - accuracy: 0.7782 - val_loss: 0.3935 - val_accuracy: 0.8202\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 157s 251ms/step - loss: 0.3605 - accuracy: 0.8448 - val_loss: 0.3917 - val_accuracy: 0.8274\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 150s 240ms/step - loss: 0.3449 - accuracy: 0.8641 - val_loss: 0.4575 - val_accuracy: 0.8066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9abe5eded0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_size = int(len(y_train)*0.2)\n",
    "num_epochs = 3\n",
    "X_valid, y_valid = X_train[:valid_size], y_train[:valid_size]\n",
    "X_train2, y_train2 = X_train[valid_size:], y_train[valid_size:]\n",
    "model_lstm.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 61s 78ms/step - loss: 0.4499 - accuracy: 0.8100\n",
      "Test accuracy: 0.8099600076675415\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model using model.evaluate()\n",
    "scores = model_lstm.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm=model_lstm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBtRY9jmI5VQ"
   },
   "source": [
    "## Perform Error analysis and explain using few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nrs2ylDaI5VR"
   },
   "outputs": [],
   "source": [
    "dict_arr={\"text\":[],\"actual_pred\":list([int(x) for x in y_test]),\n",
    "          \"rnn_prob\":list([float(x[0]) for x in y_pred_rnn]), \n",
    "          \"lstm_prob\":list([float(x[0]) for x in y_pred_lstm])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing to retrieve text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS= vocabulary_size \n",
    "INDEX_FROM=3\n",
    "\n",
    "train,test = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "train_x,train_y = train\n",
    "test_x,test_y = test\n",
    "\n",
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_text(review_idx):\n",
    "    ans=' '.join(id_to_word[id] for id in train_x[review_idx])\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_test_len=len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_id in range(tot_test_len):\n",
    "    the_text=fetch_text(curr_id)\n",
    "    dict_arr['text'].append(the_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_arr['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_stored.json\",'w') as fd:\n",
    "    json.dump(dict_arr, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(dict_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           <START> this film was just brilliant casting l...\n",
       "actual_pred                                                    0\n",
       "rnn_prob                                                0.307954\n",
       "lstm_prob                                               0.100013\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rnn_pred'] = df.apply(lambda row: 0 if row['rnn_prob'] < 0.5 else 1, axis=1)\n",
    "df['lstm_pred'] = df.apply(lambda row: 0 if row['lstm_prob'] < 0.5 else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>actual_pred</th>\n",
       "      <th>rnn_prob</th>\n",
       "      <th>lstm_prob</th>\n",
       "      <th>rnn_pred</th>\n",
       "      <th>lstm_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;START&gt; this film was just brilliant casting l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307954</td>\n",
       "      <td>0.100013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;START&gt; big hair big &lt;UNK&gt; bad music and a gia...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845825</td>\n",
       "      <td>0.822048</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;START&gt; this has to be one of the worst films ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.298401</td>\n",
       "      <td>0.186823</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;START&gt; the &lt;UNK&gt; &lt;UNK&gt; at storytelling the tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428570</td>\n",
       "      <td>0.448783</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;START&gt; worst mistake of my life br br i picke...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.653494</td>\n",
       "      <td>0.879747</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>&lt;START&gt; this is a racist movie but worthy of s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409460</td>\n",
       "      <td>0.897186</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>&lt;START&gt; bela lugosi plays a doctor who will do...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.607366</td>\n",
       "      <td>0.832184</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>&lt;START&gt; in a far away &lt;UNK&gt; is a planet called...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.506651</td>\n",
       "      <td>0.301756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>&lt;START&gt; six &lt;UNK&gt; had me hooked i looked forwa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.462248</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>&lt;START&gt; as a big fan of the original film it's...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.518565</td>\n",
       "      <td>0.881460</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  actual_pred  \\\n",
       "0      <START> this film was just brilliant casting l...            0   \n",
       "1      <START> big hair big <UNK> bad music and a gia...            1   \n",
       "2      <START> this has to be one of the worst films ...            1   \n",
       "3      <START> the <UNK> <UNK> at storytelling the tr...            0   \n",
       "4      <START> worst mistake of my life br br i picke...            1   \n",
       "...                                                  ...          ...   \n",
       "24995  <START> this is a racist movie but worthy of s...            1   \n",
       "24996  <START> bela lugosi plays a doctor who will do...            1   \n",
       "24997  <START> in a far away <UNK> is a planet called...            0   \n",
       "24998  <START> six <UNK> had me hooked i looked forwa...            0   \n",
       "24999  <START> as a big fan of the original film it's...            0   \n",
       "\n",
       "       rnn_prob  lstm_prob  rnn_pred  lstm_pred  \n",
       "0      0.307954   0.100013         0          0  \n",
       "1      0.845825   0.822048         1          1  \n",
       "2      0.298401   0.186823         0          0  \n",
       "3      0.428570   0.448783         0          0  \n",
       "4      0.653494   0.879747         1          1  \n",
       "...         ...        ...       ...        ...  \n",
       "24995  0.409460   0.897186         0          1  \n",
       "24996  0.607366   0.832184         1          1  \n",
       "24997  0.506651   0.301756         1          0  \n",
       "24998  0.462248   0.012618         0          0  \n",
       "24999  0.518565   0.881460         1          1  \n",
       "\n",
       "[25000 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOME ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rnn_lstm'] = df.apply(lambda row: 0 if row['rnn_pred']!=row['lstm_pred'] else 1, axis=1)\n",
    "df['rnn_actual'] = df.apply(lambda row: 0 if row['rnn_pred']!=row['actual_pred'] else 1, axis=1)\n",
    "df['lstm_actual'] = df.apply(lambda row: 0 if row['actual_pred']!=row['lstm_pred'] else 1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of examples where both RNN and LSTM predicted wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df=df[(df['rnn_actual']==0) & (df['lstm_actual']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148\n"
     ]
    }
   ],
   "source": [
    "print(len(dummy_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN WRONG, LSTM CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df=df[(df['rnn_actual']==0) & (df['lstm_actual']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7401\n"
     ]
    }
   ],
   "source": [
    "print(len(dummy_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM WRONG, RNN CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df=df[(df['rnn_actual']==1) & (df['lstm_actual']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2603\n"
     ]
    }
   ],
   "source": [
    "print(len(dummy_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOTH RNN, LSTM CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df=df[(df['rnn_actual']==1) & (df['lstm_actual']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12848\n"
     ]
    }
   ],
   "source": [
    "print(len(dummy_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above figure, we see there is a lot of examples (7401) to be exact where LSTM predicts correctly but RNN does not \n",
    "\n",
    "ALso, LSTM has 80% accuracy whereas RNN has 61% accuracy only on the TEST SET.\n",
    "Clearly, LSTM performs much better than RNN.\n",
    "This is because \n",
    "addition of LSTMs helps normal RNNs to remember inputs over a long period of time. This is because LSTMs contain information in a memory.\n",
    "Simple RNN fail to store information for a longer period of time. Sometimes data from long ago is required to predict the current output. But RNNs are absolutely incapable of handling such “long-term dependencies”. \n",
    "LSTMs help establish finer control over which part of the context needs to be carried forward and what fraction of the past needs to be forgotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check=df[(df['rnn_actual']==0) & (df['lstm_actual']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           <START> detective tony <UNK> frank sinatra ret...\n",
       "actual_pred                                                    0\n",
       "rnn_prob                                                0.615635\n",
       "lstm_prob                                               0.030516\n",
       "rnn_pred                                                       1\n",
       "lstm_pred                                                      0\n",
       "rnn_lstm                                                       0\n",
       "rnn_actual                                                     0\n",
       "lstm_actual                                                    1\n",
       "Name: 35, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.iloc[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the below example, RNN predicts POSITIVE REVIEW WITH 60% probability whereas LSTM predicts CORRECT neg review with 97% probability.\n",
    "\n",
    "Some keywords indicating a newgative review are:-\n",
    "* Sentence 1: sinatra tries hard to sell us the lame jokes\n",
    "* Sentence 2:  project the frankly laughable\n",
    "\n",
    "A \"laughable\" in sentence 2 captures that the director was not very good ans his efforts were laughable. \n",
    "Now, laughable could also mean that the movie was a comedy and we have a very laughable experience (nice comedy).\n",
    "\n",
    "So, \"laughable\" can be used in both senses.\n",
    "\n",
    "However, LSTM may have been able to link the \"laughable\" with \"LAME JOKES\" (negative feeling) and hence, predicts correctly,\n",
    "\n",
    "But RNN would have forgotten about \"lame\" and interprets laughable in a positive way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> detective tony <UNK> frank sinatra returns to the screen after his self titled debut this time it's a film that's played for <UNK> while on a <UNK> trip <UNK> finds the body of a blonde beauty at the bottom of the sea her feet as you might expect <UNK> in <UNK> <UNK> immediately on the case after being hired by man mountain <UNK> <UNK> <UNK> finds himself immediately at risk as he has to investigate some mafia types who turn the <UNK> on him and he is himself found to be the main suspect he must now go on the run and hope to solve the case alone the <UNK> sinatra tries hard to sell us the lame jokes and make us believe he is a good detective oh and not to mention being sexually attractive to the <UNK> <UNK> <UNK> but he fails miserably in this ham <UNK> <UNK> project the frankly laughable <UNK> that <UNK> every female is quite <UNK> every woman in the film is a <UNK> head who likes <UNK> over is front of the camera director douglas of course <UNK> in <UNK> in on the <UNK> of their <UNK> each time as they <UNK> their <UNK> <UNK> there's even a ridiculously campy gay character that <UNK> belief this was a film made by real men for real men to <UNK> their own <UNK> sexuality it's a <UNK> <UNK>\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.iloc[9]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example where LSTM fails also\n",
    "Here, \"low grade\" meant low budget.\n",
    "Also, the text does have negative terms but they are related to THE PLOT OF THE MOVIE AND NOT TO THE REVIEW of the movie. BOTH, LSTM AND RNN FAIL TO CAPTURE THIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check=df[(df['rnn_actual']==0) & (df['lstm_actual']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           <START> this low grade universal <UNK> has jus...\n",
       "actual_pred                                                    1\n",
       "rnn_prob                                                0.417258\n",
       "lstm_prob                                               0.393616\n",
       "rnn_pred                                                       0\n",
       "lstm_pred                                                      0\n",
       "rnn_lstm                                                       1\n",
       "rnn_actual                                                     0\n",
       "lstm_actual                                                    0\n",
       "Name: 52, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this low grade universal <UNK> has just been <UNK> as an <UNK> dvd release but intended as part of a collection of similar movies that i already had in my <UNK> i decided to <UNK> it from other channels rather than wait for that <UNK> release which is just as well since the end result was not anything particularly special if <UNK> atmospheric at that for <UNK> the plot is pretty weak \\x96 even though in a way it <UNK> the vincent price vehicle theatre of blood 1973 <UNK> without any of that film's campy <UNK> what we have here in fact is a <UNK> <UNK> martin <UNK> \\x96 whom we even see <UNK> his <UNK> <UNK> of cheese with his pet cat \\x96 who upon finding himself on the <UNK> end of art critic alan <UNK> <UNK> <UNK> one time too many decides to end it all by <UNK> himself into the nearby river however while <UNK> just that action he is <UNK> by <UNK> <UNK> escaped killer dubbed the <UNK> and naturally enough saves the poor <UNK> life with the intention of having the latter do all the dirty work for him in <UNK> although it is supposedly set in the art <UNK> of new york all we really see at work is <UNK> and commercial <UNK> robert <UNK> who keeps painting the same <UNK> blonde girl joan <UNK> over and over in <UNK> <UNK> \\x96 how is that for art who <UNK> enough is engaged to a rival art critic virginia grey of <UNK> before long the latter is discovered with his <UNK> broken and <UNK> is <UNK> but then <UNK> detective bill <UNK> gets the bright idea of engaging another critic to <UNK> a <UNK> review of <UNK> work i did not know that <UNK> <UNK> got <UNK> so as to <UNK> how violent his reaction is going to be in the <UNK> <UNK> <UNK> himself into thinking that he is creating his masterpiece by <UNK> <UNK> <UNK> <UNK> \\x96 and <UNK> \\x96 <UNK> which needless to say <UNK> the attention of the constantly <UNK> grey we are led to believe that she lacks material for her <UNK> <UNK> <UNK> to the <UNK> of both artist and model <UNK> although the <UNK> is fully aware of how grey looks thanks to her aforementioned haunting of <UNK> <UNK> <UNK> <UNK> he <UNK> off <UNK> \\x96 who had by then become <UNK> girl \\x96 in <UNK> apartment and <UNK> <UNK> talking to you guessed it grey about his intention to <UNK> him as the fall guy for the police sends the slow <UNK> giant off his deep <UNK> down to destroying his own now <UNK> <UNK> image <UNK> enough although this was <UNK> <UNK> film his name in the credits is <UNK> by the <UNK> <UNK>\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.iloc[2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
